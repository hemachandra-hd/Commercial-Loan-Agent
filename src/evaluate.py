import csv
import json
import boto3
from loan_agent import ask_claude

# Initialize Bedrock for the Judge
# Ensure you have access to the Llama 3 model in your AWS Bedrock console
bedrock = boto3.client(service_name="bedrock-runtime", region_name="us-east-1")

def get_judge_score(question, ai_answer, human_truth):
    """
    Uses Meta Llama 3 to act as a 'Judge' comparing the AI's answer to the Human's correction.
    """
    
    # 1. Format the prompt using Llama 3 specific special tokens
    # Llama 3 expects <|begin_of_text|>, <|start_header_id|>, etc.
    llama_prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

    You are an impartial AI Judge. 
    
    TASK: compare the AI_RESPONSE against the HUMAN_TRUTH.
    
    INPUT DATA:
    <question>
    {question}
    </question>
    
    <ai_response>
    {ai_answer}
    </ai_response>
    
    <human_truth>
    {human_truth}
    </human_truth>
    
    INSTRUCTIONS:
    - If the AI_RESPONSE agrees with the HUMAN_TRUTH, score it 'PASS'.
    - If the AI_RESPONSE contradicts the HUMAN_TRUTH, score it 'FAIL'.
    - Provide a 1-sentence explanation.
    
    FORMAT:
    Score: [PASS/FAIL]
    Reason: [Explanation]
    <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    """
    
    # 2. Construct Payload for Meta Llama 3
    # Llama 3 uses "prompt" (not messages) and "max_gen_len"
    body = json.dumps({
        "prompt": llama_prompt,
        "temperature": 0.1,  # Low temperature makes the judge strict and consistent
        "top_p": 0.9,
        "max_gen_len": 512
    })
    
    try:
        # 3. Invoke Model
        response = bedrock.invoke_model(
            modelId="meta.llama3-70b-instruct-v1:0",
            body=body
        )
        
        response_body = json.loads(response.get("body").read())
        
        # 4. Extract result (Llama 3 puts the text in 'generation')
        return response_body["generation"]
        
    except Exception as e:
        return f"Error invoking Judge: {str(e)}"

def run_evaluation():
    print("‚öñÔ∏è  STARTING AUTOMATED EVALUATION (Judge: Meta Llama 3)...\n")
    
    try:
        # Open the log file generated by your main app
        with open("feedback_log.csv", mode='r') as file:
            reader = csv.DictReader(file)
            rows = list(reader)
    except FileNotFoundError:
        print("‚ùå No feedback log found. Run the app and log some feedback first!")
        return

    pass_count = 0
    total_count = 0

    for row in rows:
        # We only evaluate cases where the human said it was "Negative" (wrong)
        if row["Rating"] == "Negative":
            total_count += 1
            print(f"üîπ Testing Case: {row['Applicant']}")
            
            # 1. Reconstruct the Query exactly as the agent sees it
            query = f"""
            New Loan Application Review:
            - Company: {row['Applicant']}
            - Industry: 'Other' (Re-testing)
            - Loan Amount: ${row['Loan_Amount']}
            - Credit Score: {row['Credit_Score']}
            - Details: {row['Details']}
            Based on the Credit Policy, should this be approved?
            """
            
            # 2. Get Fresh Answer from Agent (The code you are testing)
            # This calls the ask_claude function from loan_agent.py
            print("   ü§ñ Agent is thinking...")
            new_answer = ask_claude(query)
            
            # 3. Judge It
            print("   üë©‚Äç‚öñÔ∏è Asking Llama 3 to Judge...")
            grade = get_judge_score(query, new_answer, row["Human_Correction"])
            print(f"   üìù Judge's Report:\n{grade}\n")
            
            # Check for PASS in the judge's output
            if "Score: PASS" in grade:
                pass_count += 1

    print("-" * 30)
    if total_count > 0:
        print(f"üèÅ Final Results: {pass_count}/{total_count} Passed")
    else:
        print("üèÅ No 'Negative' feedback cases found to test.")

if __name__ == "__main__":
    run_evaluation()